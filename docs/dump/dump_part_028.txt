*   **Context Window:** Gemini's potentially larger context window allows agents to maintain context over longer interactions or process larger documents/histories, reducing the need for complex summarization or state-caching logic in some scenarios.

### **1.3 LangChain Framework Issues**

Frameworks like CrewAI, built upon LangChain, often face criticism regarding:

*   **Token Inefficiency:** LangChain's abstractions, prompt chaining, and internal state management can lead to higher token consumption compared to more direct API interactions or frameworks like Agency Swarm.
*   **Reliability/Complexity:** Hidden abstractions, complex internal logic, and frequent updates in LangChain can sometimes lead to unexpected behavior, difficult debugging, and maintenance challenges for production systems.
*   **Lack of Control:** Developers may find it harder to precisely control prompts and agent behavior compared to frameworks offering more direct access.

Agency Swarm was designed specifically to avoid these LangChain pitfalls by offering direct prompt control and leveraging the more structured (though OpenAI-specific) Assistants API. This makes it, conceptually, a better foundation for potentially adapting to other robust models like Gemini, assuming the compatibility challenges can be addressed.

### **1.4 Gemini Tools: Usage and Creation**

Gemini interacts with external tools via **Function Calling**.

**Steps:**

1.  **Define the Tool Schema:** Describe your function(s) using a JSON schema (subset of OpenAPI 3.0 schema). This tells Gemini what the function does, its parameters, and which are required.

    ```python
    import google.generativeai as genai

    # Example Tool: Get Current Weather
    get_current_weather = genai.protos.FunctionDeclaration(
        name="get_current_weather",
        description="Get the current weather in a given location",
        parameters=genai.protos.Schema(
            type=genai.protos.Type.OBJECT,
            properties={
                'location': genai.protos.Schema(
                    type=genai.protos.Type.STRING,
                    description="The city and state, e.g. San Francisco, CA"
                ),
                'unit': genai.protos.Schema(
                    type=genai.protos.Type.STRING,
                    enum=["celsius", "fahrenheit"], # Example enum
                    description="The temperature unit"
                )
            },
            required=["location"]
        )
    )
    ```

2.  **Implement the Tool Function:** Write the actual Python code that performs the tool's action.

    ```python
    import random

    def get_weather(location: str, unit: str = "celsius"):
        """Placeholder function for getting weather."""
        print(f"API CALL: Getting weather for {location} in {unit}")
        # In a real scenario, call a weather API here
        temperature = random.randint(0, 30) if unit == "celsius" else random.randint(32, 86)
        return {
            "location": location,
            "temperature": temperature,
            "unit": unit,
            "conditions": random.choice(["Sunny", "Cloudy", "Rainy"])
        }

    # Map function names to implementations
    available_tools = {
        "get_current_weather": get_weather
    }
    ```

3.  **Include Tool in API Call:** Pass the function declaration(s) to the Gemini model when generating content.

    ```python
    genai.configure(api_key="YOUR_GEMINI_API_KEY")
    model = genai.GenerativeModel(
        model_name='gemini-1.5-flash', # Or 1.5-pro, etc.
        tools=[get_current_weather] # Pass the list of declarations
    )
    chat = model.start_chat(enable_automatic_function_calling=False) # Manual control initially

    prompt = "What's the weather like in London?"
    response = chat.send_message(prompt)
    ```

4.  **Handle Function Call Response:** If Gemini decides to use a tool, its response will contain a `FunctionCall` object.

    ```python
    print(response.candidates[0].content.parts[0])
    # Example Output might look like:
    # function_call {
    #   name: "get_current_weather"
    #   args {
    #     fields {
    #       key: "location"
    #       value {
    #         string_value: "London"
    #       }
    #     }
    #   }
    # }

    function_call = response.candidates[0].content.parts[0].function_call
    ```

5.  **Execute the Function:** Use the name and arguments from the `FunctionCall` to execute your Python function.

    ```python
    if function_call.name in available_tools:
        function_to_call = available_tools[function_call.name]
        # Convert protobuf arguments to Python dict if needed, or access directly
        args = {key: getattr(value, value.WhichOneof('kind')) for key, value in function_call.args.items()}

        function_response_data = function_to_call(**args)
        print("Function Response:", function_response_data)
    else:
        # Handle case where the function name is not found
        print(f"Error: Function {function_call.name} not found.")
        function_response_data = {"error": f"Function {function_call.name} not available"}

    ```

6.  **Send Function Response Back to Gemini:** Send the result of your function execution back to the model so it can formulate a natural language response.

    ```python
    response = chat.send_message(
        genai.protos.Part(
            function_response=genai.protos.FunctionResponse(
                name=function_call.name,
                response=function_response_data # Pass the Python dict directly
            )
        )
    )

    # Final natural language response from Gemini
    print(response.text)
    # Example: The current weather in London is 15Â°C and Cloudy.
    ```

*(Note: The exact structure of `function_call.args` might vary slightly depending on the SDK version. Refer to the latest `google-genai` SDK documentation and examples.)*

### **1.5 Gemini RAG/RAT: Implementation**

**Retrieval-Augmented Generation (RAG):**

RAG enhances LLM responses by retrieving relevant information from external knowledge sources (like documents in a vector database) and providing it as context in the prompt.

**Basic RAG Steps with Gemini:**

1.  **Index Data:** Load documents, split them into chunks, generate embeddings (using Gemini's embedding models like `text-embedding-004` or other models), and store them in a vector database (e.g., ChromaDB, Pinecone, Vertex AI Vector Search).
2.  **Retrieve:** When a query comes in, generate an embedding for the query and perform a similarity search in the vector database to find the most relevant document chunks.
3.  **Augment:** Construct a prompt for Gemini that includes the original query and the retrieved document chunks as context.
4.  **Generate:** Send the augmented prompt to Gemini to get a context-aware response.

```python
# --- Conceptual RAG Example ---
import google.generativeai as genai
# Assume 'vector_store' is an initialized vector database client (e.g., ChromaDB)
# Assume 'retrieve_relevant_chunks' is a function that queries the vector_store

genai.configure(api_key="YOUR_GEMINI_API_KEY")
model = genai.GenerativeModel('gemini-1.5-flash') # Or other suitable model

def retrieve_relevant_chunks(query, k=3):
    # Placeholder: Replace with actual vector store retrieval logic
    print(f"VECTOR DB: Retrieving chunks for query: '{query}'")
    # Example result structure
    return [
        {"text": "Paris is the capital of France, known for the Eiffel Tower.", "source": "doc1.txt"},
        {"text": "The Louvre Museum in Paris houses famous art.", "source": "doc2.txt"},
        {"text": "French cuisine includes dishes like croissants and coq au vin.", "source": "doc3.txt"}
    ]

def generate_rag_response(query: str):
    # 1. Retrieve
    relevant_chunks = retrieve_relevant_chunks(query)

    # 2. Augment
    context = "\n".join([chunk['text'] for chunk in relevant_chunks])
    prompt = f"""Based on the following context, answer the question.

Context:
{context}

Question: {query}

Answer:"""

    # 3. Generate
    response = model.generate_content(prompt)

    # Optional: Include sources
    sources = list(set(chunk['source'] for chunk in relevant_chunks))
    return {"answer": response.text, "sources": sources}

# --- Usage ---
user_query = "What is Paris known for?"
rag_result = generate_rag_response(user_query)
print(f"Answer: {rag_result['answer']}")
print(f"Sources: {rag_result['sources']}")
# Example Output:
# Answer: Based on the context, Paris is known for the Eiffel Tower and the Louvre Museum, which houses famous art.
# Sources: ['doc1.txt', 'doc2.txt']
```

**Retrieval-Augmented Thinking (RAT):**

RAT extends RAG by enabling the model to perform multi-step reasoning *using* the retrieved information. This is less about direct implementation frameworks and more about structuring prompts and potentially multiple interactions with Gemini.

**RAT Strategy Example:**

1.  **Retrieve Initial Context:** Use RAG to get initial relevant chunks.
2.  **Generate Reasoning Plan:** Ask Gemini (with the initial context) to outline the steps needed to answer the query. *This might involve identifying sub-questions or needed calculations.*
3.  **Iterative Retrieval/Execution (if needed):**
    *   If the plan requires more specific information, perform targeted retrievals based on the sub-questions.
    *   If the plan requires calculations or tool use, invoke Gemini's function calling.
4.  **Synthesize Final Answer:** Provide Gemini with the original query, all retrieved context, the reasoning plan, and any intermediate results (from tool calls or further retrievals), and ask it to generate the final, reasoned answer.

Gemini's large context window is particularly beneficial for RAT, as it can hold more retrieved information and reasoning steps simultaneously.

### **1.6 Gemini MCP: Usage and Creation**

Gemini doesn't *natively implement* the MCP client/server protocol itself. However, it can **interact with MCP servers** using its **Function Calling** capability.

**Usage:**

1.  An MCP server exposes its tools/resources via a defined protocol (usually JSON-RPC over WebSockets).
2.  You define Gemini function declarations (schemas) that **mirror the tools available on the MCP server**.
3.  When Gemini returns a `FunctionCall` for one of these mirrored tools, your client code **translates this into an MCP request** (e.g., a `callTool` JSON-RPC message) and sends it to the MCP server.
4.  The response from the MCP server is then translated back into the format Gemini expects for a `FunctionResponse` and sent back to the Gemini model.

**Creation:**

Creating an MCP server *for* Gemini involves building a standard MCP server (using the MCP Python SDK or other implementations) where the **internal logic of the server's tools calls the Gemini API**.

*   **Example:** An MCP server could have a tool named `summarize_text_with_gemini`. When this tool is called via MCP, the server's implementation would take the input text, call the Gemini `generate_content` API with a summarization prompt, and return the Gemini-generated summary via the MCP response.

*(Section 4 will provide detailed code examples for creating MCP servers using the Python SDK and integrating them.)*

### **1.7 Tool/MCP Conversion (Claude -> Gemini)**

Converting tools or MCP interactions designed for Anthropic Claude to Gemini involves mapping the tool definition and interaction flow.

**Key Mapping Points:**

*   **Tool Definition:**
    *   Claude's `input_schema` directly maps to Gemini's `parameters` schema within the `FunctionDeclaration`. The structure (type, properties, description, required) is largely compatible.
*   **Interaction Flow:**
    *   **Claude:** Tool use is embedded within user/assistant messages (`tool_use` request, `tool_result` response).
    *   **Gemini:** Follows the Function Calling flow (LLM returns `FunctionCall` -> Client executes -> Client sends `FunctionResponse` back).
*   **Conversion Utility:**

    ```python
    def convert_claude_tools_to_gemini_declarations(claude_tools: list) -> list:
        """Converts a list of Claude tool definitions to Gemini FunctionDeclarations."""
        import google.generativeai as genai # Assuming SDK is imported

        gemini_declarations = []
        for tool in claude_tools:
            # Basic conversion, might need refinement for complex types/enums
            parameters_schema = genai.protos.Schema()
            if "input_schema" in tool and tool["input_schema"]:
                parameters_schema.type = genai.protos.Type.OBJECT # Assuming object type
                if "properties" in tool["input_schema"]:
                    for name, prop in tool["input_schema"]["properties"].items():
                        # Basic type mapping - needs expansion for arrays, nested objects etc.
                        prop_type_str = prop.get("type", "STRING").upper()
                        try:
                            prop_type = getattr(genai.protos.Type, prop_type_str)
                        except AttributeError:
                            prop_type = genai.protos.Type.STRING # Default fallback

                        parameters_schema.properties[name] = genai.protos.Schema(
                            type=prop_type,
                            description=prop.get("description", "")
                        )
                        # Handle enums if present
                        if "enum" in prop:
                            parameters_schema.properties[name].enum.values.extend(prop["enum"])

                if "required" in tool["input_schema"]:
                    parameters_schema.required.extend(tool["input_schema"]["required"])

            declaration = genai.protos.FunctionDeclaration(
                name=tool["name"],
                description=tool.get("description", ""),
                parameters=parameters_schema
            )
            gemini_declarations.append(declaration)
        return gemini_declarations

    # --- Example Usage ---
    # claude_tools_list = [
    #     {
    #         "name": "search_products",
    #         "description": "Search for products",
    #         "input_schema": {
    #             "type": "object",
    #             "properties": {
    #                 "query": {"type": "string", "description": "Search term"},
    #                 "category": {"type": "string", "description": "Optional category"}
    #             },
    #             "required": ["query"]
    #         }
    #     }
    # ]
    # gemini_func_declarations = convert_claude_tools_to_gemini_declarations(claude_tools_list)
    # print(gemini_func_declarations)
    ```

**Challenges:**

*   **System Prompts:** Claude uses system prompts extensively for tool guidance. Gemini lacks a dedicated system prompt field; instructions must be included in the user message history.
*   **Streaming/Partial Results:** Advanced Claude features like streaming tool results are not directly mapped in Gemini's standard function calling.

### **1.8 Coherent Multi-Agent Integration**

Successfully integrating Gemini into multi-agent systems like Agency Swarm requires addressing:

*   **State Management:** Implementing robust mechanisms to track conversation history and agent state, given Gemini's stateless API.
*   **Communication:** Defining clear protocols for inter-agent messaging (e.g., using an event bus or direct calls, potentially managed by the framework).
*   **Tool Orchestration:** Managing the function calling loop reliably across multiple agents.
*   **Compatibility Layers:** Using tools like LiteLLM or custom adapters to bridge differences between Gemini's API and the framework's expected interface (like Agency Swarm's OpenAI compatibility).

*(Detailed strategies and code examples for these patterns will be explored in subsequent sections.)*

---

## **2. Agency Swarm Framework**

Agency Swarm is an open-source framework developed by VRSEN, designed for building and orchestrating autonomous AI agents that can collaborate on complex tasks. It provides structures for defining agent roles, capabilities (tools), communication flows, and state management.

### **2.1 Compatibility & Design (OpenAI Focus)**

Agency Swarm was intentionally designed with **OpenAI models and the Assistants API as its primary backend.** Key reasons for this architectural choice include:

*   **Leveraging OpenAI Assistants API:** The framework heavily utilizes the features of the OpenAI Assistants API, such as:
    *   **Persistent Threads:** Built-in management of conversation history for stateful interactions.
    *   **Managed Tool Execution:** Simplification of the function calling loop.
    *   **Built-in Tools:** Easy integration of Code Interpreter and File Search (Retrieval).
    These features provided a robust foundation for multi-agent orchestration without requiring extensive custom implementation for core state and tool management.
*   **Mature Function Calling:** OpenAI's function calling was well-established and reliable at the time of Agency Swarm's development, crucial for agent-tool interaction.
*   **Avoiding LangChain:** The developer explicitly avoided dependencies on LangChain to maintain simplicity, ensure greater control over prompts and agent behavior, improve performance by reducing abstraction layers, and minimize potential maintenance issues associated with LangChain's complexity and rapid evolution.

This focus on the OpenAI ecosystem means Agency Swarm provides a streamlined experience when using GPT models but requires adaptation for other LLMs like Gemini.

### **2.2 Challenges Adapting Agency Swarm to Gemini (Assistants API Incompatibility)**

Directly using Google Gemini models within Agency Swarm presents **significant challenges** due to the framework's fundamental reliance on the **stateful OpenAI Assistants API structure**.
