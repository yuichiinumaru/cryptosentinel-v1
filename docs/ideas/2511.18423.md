# Arxiv Analysis: General Agentic Memory Via Deep Research

**ID:** 2511.18423
**Date:** 2024-07-29
**Link:** https://arxiv.org/abs/2511.18423

## Executive Summary
The paper introduces **General Agentic Memory (GAM)**, a novel memory framework that operates on a "Just-in-Time" (JIT) compilation principle, contrasting with the common "Ahead-of-Time" (AOT) approach. Instead of pre-compressing information into a static memory (which inevitably causes information loss), GAM maintains a lossless, complete history in a **Page-Store**. At runtime, a dedicated **Researcher** agent performs iterative "deep research"—a cycle of planning, searching, and reflecting—to dynamically retrieve and synthesize the precise context needed for a given task. This is guided by a lightweight **Memorizer** that creates high-level summaries offline. This dual-agent system allows GAM to handle complex, fine-grained queries and adapt to unforeseen information needs more effectively than traditional memory systems.

## Idea Brainstorming
*What concepts are potentially useful for THIS project?*
- **JIT vs. AOT Paradigm Shift:** The core conceptual shift from pre-storing summarized memories to storing full-fidelity history and researching it on-demand is the most critical idea. It addresses the limitation that our current `KhalaMemoryToolkit` loses granular detail.
- **Dual Storage System:** The idea of having a lightweight, searchable "memo" database (our current `KhalaMemory`) to guide searches into a complete, lossless "Page-Store" (which we would need to build) is a powerful and practical architecture.
- **Dedicated Researcher Agent:** Introducing a new agent whose sole responsibility is to answer complex historical questions by iteratively querying the Page-Store. This elevates memory retrieval from a simple tool call to a sophisticated, delegated reasoning task.
- **Multi-faceted Search Toolkit:** The Researcher agent should be equipped with multiple ways to query the Page-Store (e.g., semantic/vector search, keyword/BM25 search, direct ID/temporal search), as proposed in the paper. This improves the chances of finding relevant information.

## Gap Analysis
*Compare paper concepts vs. `current_codebase`. What do we already have? What is missing?*

| Feature/Concept | Current State (Codebase) | Missing / Needed |
| :--- | :--- | :--- |
| **Memory Paradigm** | **Ahead-of-Time (AOT):** Agents proactively store summaries in `KhalaMemoryToolkit` via `store_memory`. Information is compressed *before* it's needed. | A **Just-in-Time (JIT)** approach is needed. The system must be re-architected to process information *at query time*. |
| **Storage** | **Single-Layer:** `KhalaMemoryToolkit` stores memories in a single SurrealDB table. The full context of why the memory was stored is lost. | **Dual-Layer Storage:** The project is missing a **Page-Store** for lossless, complete historical agent session data (e.g., thoughts, tool calls, observations). |
| **Memorization** | **Ad-hoc & Lossy:** Any agent can decide to store a memory summary. The process is unstructured and the full context is discarded. | A dedicated **Memorizer** role/process is needed to systematically process raw session history into pages (for the Page-Store) and lightweight memos (for the existing Khala memory). |
| **Retrieval** | **Simple Search:** A single tool call (`search_memory`) performs a one-shot BM25 search on the pre-computed summaries. It is not iterative. | **Agentic Research:** A **Researcher** agent with an iterative `plan -> search -> reflect` loop is completely absent. |
| **Tools** | **Single Retrieval Tool:** `khala_integration.search_memory`. | A **ResearchToolkit** with multiple search methods (vector, keyword, ID-based) for querying the lossless Page-Store is needed. |


## Implementation Plan
*Granular, step-by-step task list to port the ideas to our code.*

- [ ] **Phase 1: Foundational Layer - The Page-Store**
  - [ ] Define a new storage schema in `backend/storage/` for the "Page-Store" to hold full agent session transcripts (inputs, thoughts, tool interactions, outputs).
  - [ ] Implement a `PageStoreToolkit` with basic functions to `add_session_to_page_store`.
  - [ ] Integrate this toolkit into the main agent loop in `backend/main.py` or the agent factory to automatically record every session.

- [ ] **Phase 2: The Memorizer**
  - [ ] Create a new, simple `MemorizerAgent` whose role is to process a completed session transcript from the Page-Store.
  - [ ] This agent will generate a concise summary (a "memo") of the session.
  - [ ] The `MemorizerAgent` will then use the *existing* `KhalaMemoryToolkit.store_memory` function to save this memo. This elegantly repurposes our current memory system as the "lightweight" guiding memory.

- [ ] **Phase 3: The Researcher Agent & Toolkit**
  - [ ] Create a new `ResearcherAgent` in `backend/agents/`. Define its system prompt to follow the `plan -> search -> reflect` cycle for answering user queries about past events.
  - [ ] Develop a new `ResearchToolkit` in `backend/tools/` with multiple search tools for the Page-Store (e.g., `search_pages_by_vector`, `search_pages_by_keyword`).
  - [ ] Equip the `ResearcherAgent` with this new toolkit and the *existing* `KhalaMemoryToolkit` (for searching the lightweight memos to get its bearings).

- [ ] **Phase 4: Integration into the Cognitive Architecture**
  - [ ] Modify the primary `DeepTraderManager` (or equivalent coordinator agent) to recognize when a user query requires historical lookup.
  - [ ] Instead of calling `khala_memory.search_memory` itself, the manager will delegate the query to the new `ResearcherAgent`.
  - [ ] Add tasks to `docs/02-tasks.md` to track the implementation of this new agent and its integration.
