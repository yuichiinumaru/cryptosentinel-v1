# Arxiv Analysis: PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval

**ID:** 2511.14130
**Date:** 2024-11-25
**Link:** https://arxiv.org/abs/2511.14130

## Executive Summary
The paper introduces PRISM, a training-free framework for financial information retrieval that combines prompt engineering, In-Context Learning (ICL), and multi-agent systems (MAS). The key finding is that a non-agentic workflow, using a highly refined, reasoning-oriented prompt (`P4`) and selective ICL at the document-ranking stage, achieves the best performance. This approach outperforms more complex multi-agent systems, which suffered from coordination overhead and sensitivity to prompt-architecture compatibility. The paper highlights that precise, context-aware, and dynamically constructed prompts are critical for effective financial data retrieval.

## Idea Brainstorming
*What concepts are potentially useful for THIS project?*
- **Refined Prompting (`P4`):** The paper's most successful prompt (`P4`) combines Chain-of-Thought, ReAct, and Tree-of-Thought strategies. We can adapt this multi-strategy reasoning approach to enhance our `DynamicPrompting` system. Instead of just basic templates, the `PromptBuilder` could construct prompts that explicitly guide agents through comprehension, keyword extraction, and multi-perspective analysis before acting.
- **Selective ICL for Triage:** The paper found that ICL was most effective when used only at the document-ranking stage, not the chunk-ranking stage. This is analogous to our `Triage` layer. We could implement an ICL mechanism where the `Triage` agent retrieves semantically similar historical tasks from `khala-agentmemory` to use as few-shot examples, improving its ability to classify intent and select the correct agent team without overloading the downstream worker agents.
- **Non-Agentic Workflows for Simple Tasks:** The paper's finding that MAS can be overkill for certain tasks is a valuable lesson. We should evaluate our agent workflows to identify tasks that could be simplified into a non-agentic (single-agent) process guided by a powerful, dynamically-built prompt, reducing latency and cost.

## Gap Analysis
*Compare paper concepts vs. `current_codebase`. What do we already have? What is missing?*

| Feature/Concept | Current State (Codebase) | Missing / Needed |
| :--- | :--- | :--- |
| **Advanced Prompt Engineering** | `DynamicPrompting` (`backend/scaffolding/`) uses a `PromptBuilder` with Jinja2 templates. | The current prompts are relatively static. We need to implement the multi-strategy reasoning patterns (CoT, ReAct, ToT) from the paper's `P4` prompt. The `PromptBuilder` needs logic to dynamically assemble these more complex reasoning structures. |
| **In-Context Learning (ICL)** | `khala-agentmemory` exists for long-term memory, but there is no explicit ICL retrieval mechanism for prompting. | A dedicated service is needed to query `khala-agentmemory` for semantically similar, successful historical task examples. This service would format these examples and inject them into the prompt via the `ContextInjector` for the `Triage` agent. |
| **Agentic vs. Non-Agentic Flow** | The architecture is heavily agentic, with a `Triage` agent routing to specialized agent teams for all tasks. | The system lacks a mechanism to bypass the full agent team for simpler, retrieval-heavy tasks. A new "single-agent workflow" could be introduced, where the `Triage` agent delegates to a single, powerful LLM call guided by a PRISM-style prompt, rather than spawning a full team. |

## Implementation Plan
*Granular, step-by-step task list to port the ideas to our code.*

- [ ] **Phase 1: Enhance `DynamicPrompting`**
  - [ ] Create new prompt fragment templates in `backend/prompts/templates/` based on the CoT, ReAct, and ToT reasoning steps outlined in the paper's `P4` prompt.
  - [ ] Update the `PromptBuilder` in `backend/scaffolding/` to conditionally assemble these fragments into a cohesive, multi-strategy prompt.
- [ ] **Phase 2: Implement ICL for `Triage` Agent**
  - [ ] Develop a new `ICLRetriever` service that connects to `khala-agentmemory`.
  - [ ] The `ICLRetriever` will take a user query, find the top-k similar historical tasks (query + successful outcome), and format them as few-shot examples.
  - [ ] Integrate the `ICLRetriever` with the `ContextInjector` to make these examples available to the `PromptBuilder` when constructing the `Triage` agent's prompt.
- [ ] **Phase 3: Introduce a Non-Agentic Workflow**
  - [ ] Modify the `Triage` agent's logic to include a new classification: "Simple Retrieval".
  - [ ] When a task is classified as "Simple Retrieval", the `Triage` agent will bypass the `AgentSpawner` and instead execute a single, powerful LLM call using the enhanced `P4`-style prompt from Phase 1.
  - [ ] Add configuration options to define which intents are eligible for the non-agentic workflow.
