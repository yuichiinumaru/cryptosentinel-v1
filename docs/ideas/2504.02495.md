# Arxiv Analysis: Inference-Time Scaling for Generalist Reward Modeling

**ID:** 2504.02495
**Date:** 2024-05-21
**Link:** https://arxiv.org/abs/2504.02495

## Executive Summary
The paper proposes that the performance of a reward model (RM) can be significantly improved by allocating more compute at inference time, rather than solely relying on increasing model size during training. The authors introduce a Generative Reward Model (GRM) that is trained with a novel method called Self-Principled Critique Tuning (SPCT). SPCT uses online Reinforcement Learning to teach the GRM to first generate an evaluative "principle" or rule, and then critique a given response based on that principle. This creates a more robust and scalable reward signal. For inference-time scaling, they use parallel sampling (generating multiple critiques) and a meta-RM to vote on the best critique, demonstrably improving performance on RM benchmarks.

## Idea Brainstorming
*What concepts are potentially useful for THIS project?*
- **Concept 1: GRM as a "Meta-Analyst" or "Judge" Agent:** The current system's consensus mechanism is a simple `majority_vote`. We can replace this with a dedicated `JudgeAgent` built as a GRM. Instead of a simple vote, this agent would receive the analyses from the `BullResearcher` and `BearResearcher`, generate a guiding principle for the specific market context, and produce a detailed critique and a confidence score. This provides a far richer and more nuanced signal for the `DeepTraderManager`.
- **Concept 2: SPCT for the `LearningCoordinator`:** The `LearningCoordinator` agent's purpose is to improve other agents, but the mechanism is not well-defined. SPCT provides a formal framework. The `LearningCoordinator` can be re-architected to implement an SPCT loop. It would analyze the `JudgeAgent`'s critiques and the actual trade outcomes (from `evaluate_consensus_performance` tool) to refine the "principles" used for evaluation, effectively learning what constitutes a good trading argument.
- **Concept 3: Inference-Time Scaling for High-Stakes Decisions:** The paper's core idea of using more compute on demand is perfect for a trading system. For high-value or high-uncertainty trades, the system could dynamically scale by spawning multiple, diverse instances of the researcher agents (e.g., 3 Bull, 3 Bear, each with slightly different system prompts). All six analyses would then be fed to the `JudgeAgent`, which would use the paper's "parallel sampling" and "meta-voting" concepts to arrive at a much more robust, well-reasoned final decision.

## Gap Analysis
*Compare paper concepts vs. `current_codebase`. What do we already have? What is missing?*

| Feature/Concept | Current State (Codebase) | Missing / Needed |
| :--- | :--- | :--- |
| **Reward Modeling** | A `LearningCoordinator` agent exists. The `evaluate_consensus_performance` tool in `backend/tools/learning.py` provides a basic binary reward signal (win/loss). | The current reward is a simple scalar. A proper **Generative Reward Model** that produces natural language critiques and auditable principles is missing. The existing mechanism is reactive, not generative. |
| **Consensus Mechanism** | The `ConsensusToolkit` in `backend/tools/consensus.py` uses a primitive `majority_vote` function. | The system lacks a sophisticated **`JudgeAgent`** capable of scoring, critiquing, and synthesizing diverse inputs. The current voting mechanism is naive and loses significant information. |
| **Scalable Agent Reasoning**| The system is hardcoded to run a single 1v1 debate between a `BullResearcher` and `BearResearcher`. | The architecture has no mechanism to **dynamically spawn multiple agents** for parallel analysis (inference-time scaling) and intelligently aggregate their diverse outputs. |
| **Principle-Driven Learning** | Agent instructions are static markdown files. The `LearningCoordinator` is intended to update agent behavior, but lacks a formal, principle-driven method for doing so. | A formal **Self-Principled Critique Tuning (SPCT)** loop is entirely absent. The system cannot currently learn and refine its own evaluation criteria based on performance. |

## Implementation Plan
*Granular, step-by-step task list to port the ideas to our code.*

- [ ] **Phase 1: Develop a `JudgeAgent` (GRM).**
    - [ ] Create a new agent class in `backend/agents/`.
    - [ ] Its prompt will instruct it to receive multiple analyses (e.g., bull vs. bear cases), first generate a single, relevant "investment principle" for the context, and then use that principle to write a final critique and assign a score (e.g., 1-100) to the proposed action.

- [ ] **Phase 2: Redesign Consensus Flow.**
    - [ ] In `backend/main.py`, modify the "with consensus" flow.
    - [ ] Instead of calling `majority_vote`, the endpoint should gather bull/bear outputs and pass them to the new `JudgeAgent`.
    - [ ] The final decision passed to `DeepTraderManager` should be the high-quality critique from the `JudgeAgent`, not a simple voter consensus.

- [ ] **Phase 3: Implement Dynamic, Inference-Time Scaling.**
    - [ ] Update the `AgentFactory` in `backend/agents/__init__.py` to support spawning N>1 instances of researcher agents based on a request parameter (e.g., `confidence_level`).
    - [ ] Use `asyncio.gather` to run the multiple agent instances in parallel.
    - [ ] Ensure the `JudgeAgent` is equipped to handle and synthesize this larger set of inputs into a single, coherent judgment.

- [ ] **Phase 4: Evolve `LearningCoordinator` into an SPCT Engine.**
    - [ ] Refactor the `LearningCoordinator` to persist the "principles" generated by the `JudgeAgent` in Khala memory.
    - [ ] Create a new background task that is triggered after a trade's outcome is known (via `evaluate_consensus_performance`).
    - [ ] This task will feed the principle, the critique, and the outcome back to the `LearningCoordinator`, which will be prompted to either reinforce or refine the principle, thus closing the SPCT loop.
