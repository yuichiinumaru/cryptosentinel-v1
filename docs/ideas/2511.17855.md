# Arxiv Analysis: QuickLAP: Quick Language–Action Preference Learning for Autonomous Driving Agents

**ID:** 2511.17855
**Date:** 2024-07-26
**Link:** https://arxiv.org/abs/2511.17855

## Executive Summary
The paper introduces "QuickLAP," a Bayesian framework for real-time reward function learning by fusing two types of user feedback: physical corrections (actions) and natural language explanations. The core idea is that language clarifies the *intent* behind an ambiguous physical action. QuickLAP uses a dual-LLM system: an "Attention LLM" identifies which reward features the user's language is targeting, and a "Preference LLM" determines the desired change and a confidence score for that change. This structured output is then integrated into a closed-form MAP update rule, allowing the agent to adapt its understanding of the user's preferences on the fly.

## Idea Brainstorming
*What concepts are potentially useful for THIS project?*
- **Multimodal Feedback for Agent Strategy Tuning:** The core concept of using both actions (e.g., a user manually overriding a trade) and language ("That trade was too risky, focus more on Sharpe ratio") to tune the strategic parameters of the trading agents is highly valuable. This could replace or augment the current `LearningCoordinator`, which only modifies high-level instructions.
- **LLM-based Feedback Interpretation:** The dual-LLM approach for parsing user feedback is directly applicable. We could create a `FeedbackProcessingAgent` that takes a user's action and text, identifies the relevant financial metric (e.g., "volatility," "slippage," "market impact"), and determines the desired adjustment.
- **Confidence-Weighted Updates:** The idea of using an LLM to generate a *confidence score* for its own interpretation is powerful. It allows the system to differentiate between clear, actionable feedback ("increase stop-loss to 5%") and vague feedback ("be more careful"), making the learning process more robust.

## Gap Analysis
*Compare paper concepts vs. `current_codebase`. What do we already have? What is missing?*

| Feature/Concept (QuickLAP) | Current State (CryptoSentinel) | Missing / Needed |
| :--- | :--- | :--- |
| **Reward Function Representation** | Agent behavior is guided by high-level natural language instructions in `.md` files. There is no explicit, mathematical reward function (e.g., `R = θ * Features`). | A feature-based reward model must be defined. This involves identifying key performance indicators (e.g., Sharpe ratio, max drawdown, volatility, gas costs) as features `Φ` and representing agent preferences as a weight vector `θ`. |
| **Real-time User Feedback Loop** | User interaction is command-based (e.g., "buy ETH"). There is no mechanism for users to provide corrective feedback on an agent's autonomous actions. | A new UI/API endpoint is needed to capture user feedback. This requires defining what a "physical correction" means in trading (e.g., a manual trade override, canceling a pending transaction) and a way to submit accompanying text. |
| **Dual-LLM Feedback Parser** | Agents use LLMs for reasoning based on static instructions. There is no system for interpreting user *feedback* on agent *behavior*. | A new `FeedbackProcessingAgent` or toolkit is required. It would need prompts for both the `Attention` (what metric is the user talking about?) and `Preference` (what is the desired change and confidence?) roles. |
| **Bayesian MAP Update Rule** | The `LearningCoordinator` modifies agent instructions based on performance, which is a form of meta-learning, not mathematical reward inference. | The entire Bayesian update logic from the paper needs to be implemented. This involves creating a new component, likely a `PreferenceLearningToolkit`, that can take the output from the feedback parser and the "physical" action to update the `θ` vector. |
| **Online Preference Adaptation** | The system operates continuously, but the learning cycle is high-level and not designed for real-time, per-action preference updates. | The trading logic in agents like `DeepTraderManager` and `Trader` would need to be refactored to use the dynamically updated `θ` vector from the `PreferenceLearningToolkit` for every decision. |

## Implementation Plan
*Granular, step-by-step task list to port the ideas to our code.*

- [ ] **Phase 1: Foundation - Feature and Feedback Representation**
    - [ ] **Task 1.1: Define the Reward Feature Set (`Φ`).**
        - In a new file, `backend/learning/reward_features.py`, define a class that calculates a vector of key trading metrics.
        - Initial features should include: `realized_pnl`, `unrealized_pnl`, `sharpe_ratio`, `sortino_ratio`, `max_drawdown`, `volatility`, `trade_frequency`, `slippage`, and `gas_cost`.
    - [ ] **Task 1.2: Define the "Physical Correction" Data Structure.**
        - In `backend/protocol.py`, define a new Pydantic model `UserCorrection` that represents a user's corrective action.
        - This should include the original agent-proposed action, the user's overriding action, and the free-form text explanation.
    - [ ] **Task 1.3: Create a User Feedback Endpoint.**
        - In `backend/main.py`, add a new `/feedback` endpoint that accepts the `UserCorrection` model.

- [ ] **Phase 2: Core Logic - The Feedback Loop**
    - [ ] **Task 2.1: Create the `PreferenceLearningToolkit`.**
        - Create `backend/tools/preference_learning.py`.
        - Implement the Bayesian MAP update rule (Eq. 6 from the paper) as a core function `update_preference_weights(current_weights, feature_difference, language_shift, confidence, attention)`.
        - The agent preference weights (`θ`) will be stored and managed per-session, perhaps in the `SqliteStorage`.
    - [ ] **Task 2.2: Implement the Dual-LLM Feedback Parser Agent.**
        - Create a new agent directory `backend/FeedbackProcessor/`.
        - **Task 2.2.1: Create `Attention` Prompt:** Develop a prompt (`instructions_attention.md`) that takes user language and a list of reward features (from Task 1.1) and returns a JSON object with an attention mask (`r` in the paper).
        - **Task 2.2.2: Create `Preference` Prompt:** Develop a prompt (`instructions_preference.md`) that takes the user language, the feature difference, and the attention mask, and returns a JSON object with the desired shift (`μ`) and confidence (`m`).
        - **Task 2.2.3: Create the Agent:** In `backend/agents/__init__.py`, create a factory function for a `FeedbackProcessor` agent equipped with these two prompting strategies.

- [ ] **Phase 3: Integration and Refactoring**
    - [ ] **Task 3.1: Integrate Feedback Loop into `DeepTraderManager`.**
        - The `/feedback` endpoint should trigger the `DeepTraderManager`.
        - The manager will first call the `FeedbackProcessor` agent to parse the user's `UserCorrection`.
        - Then, it will use the `PreferenceLearningToolkit` to calculate the updated `θ` vector.
        - Finally, it will persist the new `θ` for the session.
    - [ ] **Task 3.2: Refactor `Trader` Agent to Use Dynamic Preferences.**
        - Modify the `Trader` agent so that before executing a trade, it retrieves the current session's `θ` vector.
        - The agent's decision-making logic must be changed to optimize for the reward function defined by `R = θ * Φ`, rather than just following static instructions. This is a major architectural change.
    - [ ] **Task 3.3: Add Unit Tests.**
        - Add tests for the `RewardFeatures` calculation.
        - Add tests for the `PreferenceLearningToolkit`, mocking the LLM outputs to ensure the MAP update is calculated correctly.
        - Add an integration test for the `/feedback` endpoint.
