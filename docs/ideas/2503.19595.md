# Arxiv Analysis: Optimizing Language Models for Inference Time Objectives using Reinforcement Learning

**ID:** 2503.19595
**Date:** 2025-03-26
**Link:** https://arxiv.org/abs/2503.19595

## Executive Summary
The paper proposes a method to improve language model performance by directly training them to optimize for inference-time objectives like `pass@k` and majority voting, instead of traditional next-token prediction. It uses Reinforcement Learning (RL) with a leave-one-out control variate to achieve this, demonstrating a trade-off where models specialized for `pass@k` excel at that metric, sometimes at a slight cost to `pass@1` performance. This approach is highly relevant for enhancing the reasoning and problem-solving capabilities of multi-agent systems.

## Idea Brainstorming
*What concepts are potentially useful for THIS project?*
- **`pass@k` Optimization for MarsRL:** The core idea is to train the `Solver` agent in the `MarsRL` system to generate `k` potential solutions. A `Verifier` agent can then check these solutions, and the reward signal for the `Solver` would be based on whether at least one of the `k` solutions is correct. This aligns perfectly with tasks that have verifiable outcomes, like code generation or mathematical reasoning.
- **Majority Voting for Debate:** The existing `BullResearcher` and `BearResearcher` agents already form a debate structure. We can extend this to a "council" of multiple researchers (e.g., 3 Bulls, 3 Bears). The `DebateCoordinator` can then use a majority vote on their final sentiment (BUY/SELL) to make a more robust decision, and the RL training can optimize the researchers' policies based on the success of the majority-voted outcome.
- **Biased Gradient Estimator:** The paper's biased but lower-variance gradient estimator (Eqn 3) could be implemented in the RL training loop for `MarsRL` agents. This could lead to more stable and efficient training, especially on complex problems where reward signals are sparse.

## Gap Analysis
*Compare paper concepts vs. `current_codebase`. What do we already have? What is missing?*

| Feature/Concept | Current State (Codebase) | Missing / Needed |
| :--- | :--- | :--- |
| `pass@k` Logic | The `MarsRL` system is a placeholder. It has the structure (Solver, Verifier, Corrector) but lacks the core RL training loop and the logic for handling multiple (`k`) outputs for `pass@k` evaluation. | The `Verifier` needs to be adapted to check multiple solutions and provide a binary reward. |
| Multi-Agent Voting | The current debate is a 1v1 (Bull vs. Bear). | There is no mechanism to spawn multiple instances of these agents for a single request or to aggregate their votes. The `AgentSpawner` and `TriageUnit` from the ARTEMIS architecture would be necessary to implement a "council" approach. |
| RL Training Loop | The core policy gradient update mechanism is not fully implemented in `MarsRL`. | The paper's proposed gradient estimators (unbiased leave-one-out and the biased variant) would need to be added to this loop. |

## Implementation Plan
*Granular, step-by-step task list to port the ideas to our code.*

- [ ] Task 1: Implement a basic PPO/RL training loop for the `Solver` agent within the `MarsRL` framework.
- [ ] Task 2: Modify the `Solver` agent's generation logic to produce `k` distinct outputs for a given problem.
- [ ] Task 3: Adapt the `Verifier` agent to evaluate `k` solutions and return a `pass@k` style reward (1 if at least one is correct, 0 otherwise).
- [ ] Task 4: Integrate the unbiased leave-one-out gradient estimator (Eqn 2 from the paper) into the RL training loop for the `Solver`.
- [ ] Task 5: (Optional/Stretch) Implement the biased gradient estimator (Eqn 3) and add a hyperparameter to switch between the two for ablation studies.
- [ ] Task 6: Design and implement the "council" structure for the research team, allowing the `DebateCoordinator` to manage multiple `BullResearcher` and `BearResearcher` agents and perform a majority vote.
