# Arxiv Analysis: Chain of Draft: Thinking Faster by Writing Less

**ID:** 2502.18600
**Date:** 2024-12-18
**Link:** https://arxiv.org/abs/2502.18600

## Executive Summary
The paper introduces "Chain of Draft" (CoD), a novel prompting strategy for Large Language Models (LLMs). It is inspired by the human cognitive process of using concise drafts or shorthand notes to solve complex problems. Unlike the verbose, step-by-step nature of Chain-of-Thought (CoT), CoD instructs the LLM to generate minimalistic yet informative intermediate reasoning steps. The authors empirically demonstrate that CoD matches or surpasses CoT in accuracy across arithmetic, commonsense, and symbolic reasoning tasks while using significantly fewer tokens (as little as 7.6%), thereby reducing both computational cost and latency.

## Idea Brainstorming
*What concepts are potentially useful for THIS project?*
- **Agent Efficiency:** The core concept of CoD can be directly applied to the reasoning cycles of all specialized agents (`MarketAnalyst`, `Trader`, `RiskAnalyst`). For time-sensitive tasks like analyzing market volatility or calculating entry/exit points, reducing latency and computational overhead is critical. By switching from verbose CoT to concise CoD, agents can "think" faster and react to market changes more quickly.
- **Triage Optimization:** The `TriageUnit` agent, which is responsible for routing and spawning other agents, could use a CoD-style prompt. Its task of intent classification does not require deep, verbose reasoning. A CoD prompt would allow it to classify the task and initiate the correct agent worker faster.
- **Tool Sequence Summarization:** When an agent executes a sequence of tools (e.g., fetching data, checking portfolio, analyzing sentiment), it can use CoD to create a condensed summary of the intermediate findings. This creates a concise "state of the world" before the agent proceeds to the next major step, improving the efficiency of its final decision-making process.
- **Memory Condensation:** The system uses `khala-agentmemory` for long-term reasoning. Agent memories and experiences could be processed with a CoD "summarizer" prompt before being committed to the vector store. This would create more information-dense and compact memory entries, potentially improving the relevance and speed of future memory retrievals.

## Gap Analysis
*Compare paper concepts vs. `current_codebase`. What do we already have? What is missing?*

| Feature/Concept | Current State (Codebase) | Missing / Needed |
| :--- | :--- | :--- |
| **Dynamic Prompting** | The system already has a `PromptBuilder` using `Jinja2` templates, as defined in `docs/01-plan.md`. This is a perfect foundation. | We need to create a new set of `PromptTemplate` files specifically designed for the CoD strategy, containing instructions for concise reasoning. |
| **Agent Spawning** | The `AgentSpawner` and `TriageUnit` in `backend/orchestration/` manage the agent lifecycle and can pass context during instantiation. | The `TriageUnit` needs logic to decide *when* to use CoD. This could be based on the task type (e.g., simple query vs. deep analysis) or a user-specified preference for speed vs. detailed reasoning. The `AgentSpawner` needs to be modified to accept a "strategy" parameter (`CoT` vs. `CoD`). |
| **Reasoning Strategy** | The current architecture implicitly supports CoT through detailed system prompts but does not have a formal, switchable strategy system. | An explicit `strategy` parameter should be added to the agent invocation flow. This would allow for runtime selection of the reasoning method, making the system more flexible. |
| **Performance Monitoring** | Basic logging exists, but there is no dedicated mechanism to measure token consumption or latency per reasoning step for performance tuning. | The agent execution loop should be instrumented to log token counts and execution time for each thought/action cycle. This would allow us to quantify the benefits of CoD and identify performance bottlenecks. |

## Implementation Plan
*Granular, step-by-step task list to port the ideas to our code.*

- [ ] **Task 1: Create CoD Prompt Templates:**
  - In `backend/prompts/templates/`, create a new subdirectory named `cod`.
  - Add new Jinja2 template files (`analyst.j2`, `trader.j2`, etc.) that include the core CoD instruction: "Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most."
- [ ] **Task 2: Enhance PromptBuilder Service:**
  - Modify the `PromptBuilder` class in `backend/scaffolding/` to accept a `strategy: str` argument (e.g., 'cod', 'cot').
  - The builder will then load templates from the corresponding directory (`cod/` or `cot/`).
- [ ] **Task 3: Update Orchestration Layer:**
  - Modify the `TriageUnit` to determine the appropriate strategy. Add a simple rule: if the task is classified as "analytics" or "trading_execution", set `strategy='cod'`.
  - Update the `AgentSpawner` to accept the `strategy` parameter and pass it to the `PromptBuilder`.
- [ ] **Task 4: Implement Performance Logging:**
  - In the main agent execution loop, add logging statements to capture and record `response.usage.total_tokens` and the latency of the LLM call.
- [ ] **Task 5: Add Integration Tests:**
  - Create a new test case in the `tests/` directory that runs the same complex query (e.g., a multi-step market analysis) twice: once with `strategy='cot'` and once with `strategy='cod'`.
  - Assert that both strategies arrive at a similar final answer, but that the token count and execution time for the CoD run are significantly lower.
