# Arxiv Analysis: Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining

**ID:** 2511.15456
**Date:** 2024-07-29
**Link:** https://arxiv.org/abs/2511.15456

## Executive Summary
The paper introduces the Transaction Intent Mining (TIM) framework, a multi-agent LLM system designed to infer user intent behind DeFi transactions. It uses a hierarchical structure of agents (Meta-Planner, Domain Experts, Question Solvers, Cognition Evaluator) to decompose the problem, gather multi-modal on- and off-chain data, and validate the findings to mitigate hallucinations. The core idea is to move beyond simple structural analysis of transactions to a deeper, semantic understanding of user motivation.

## Idea Brainstorming
*What concepts are potentially useful for THIS project?*
- **Hierarchical Agent Teams:** The concept of a Meta-Planner (MP) decomposing a complex task for specialized Domain Expert (DE) agents is directly applicable to our own agent architecture. We could use this to refine the `DeepTraderManager`'s logic, having it act as an MP that spawns temporary, specialized agents for tasks like "evaluate impermanent loss risk for a new liquidity pool" or "analyze the tokenomics of a newly launched asset."
- **Cognition Evaluator (CE):** The idea of a final "skeptic" agent that cross-validates the outputs of other agents against verifiable data is a powerful mechanism to combat LLM hallucination. We could implement a `VerificationAgent` that takes the output from the `BullResearcher` and `BearResearcher` and fact-checks their claims against on-chain data or trusted oracles before a trade is executed.
- **Intent Taxonomy:** While we don't need to replicate their exact taxonomy, the *concept* of having a structured, machine-readable taxonomy of potential user/trading intents could be very powerful. This could be used to guide the `Triage` agent's routing logic in a more structured way and help the `LearningCoordinator` categorize the success/failure of different strategies.

## Gap Analysis
*Compare paper concepts vs. `current_codebase`. What do we already have? What is missing?*

| Feature/Concept | Current State (Codebase) | Missing / Needed |
| :--- | :--- | :--- |
| **Hierarchical Agent Teams** | Our architecture uses a fixed team of agents (`MarketAnalyst`, `DeepTraderManager`, researchers) instantiated by a factory. It's a "flat" hierarchy where the `DeepTraderManager` coordinates, but doesn't dynamically spawn new, specialized agents for sub-tasks. | A true "agent spawner" or "ephermal worker" pattern as described in `01-plan.md`. The `AgentSpawner` is planned but not implemented. The TIM paper provides a concrete use-case for this pattern: intent analysis. |
| **Cognition Evaluator** | We have a `ConsensusToolkit` that performs a `majority_vote`. This is a form of evaluation, but it's simplistic and doesn't perform active, evidence-based verification. It assumes the majority is correct, which isn't always true. | A dedicated agent or toolkit with the sole responsibility of fact-checking. This would require tools that can trace claims back to their source (e.g., a specific transaction hash, an API call to a protocol's analytics page) and verify them. Our current tools fetch data, but don't explicitly have a "verify this claim" function. |
| **Intent Taxonomy** | The system operates on natural language prompts. Intent is implicitly derived by the agents, but there is no formal, structured representation of it. | A formal data structure (e.g., an Enum or a Pydantic model in `backend/config.py`) that defines possible trading intents. This would need to be integrated into the `Triage` layer and the `LearningCoordinator`'s memory. |

## Implementation Plan
*Granular, step-by-step task list to port the ideas to our code.*

- [ ] **Task 1: Define a formal `TradingIntent` Enum.**
  - Create a new file `backend/schemas/intent.py`.
  - Define a Python `Enum` with initial high-level intents like `SPECULATIVE_PROFIT`, `YIELD_FARMING`, `RISK_HEDGING`, `GOVERNANCE_PARTICIPATION`.
- [ ] **Task 2: Implement a `VerificationToolkit`.**
  - Create a new `backend/tools/verification.py`.
  - The toolkit will have a method `verify_claim(claim: str, evidence_source: str)` which can, for example, re-run a specific `MarketDataToolkit` call or check a transaction on-chain to validate an assertion made by another agent.
- [ ] **Task 3: Create a `VerificationAgent`.**
  - In `backend/agents/__init__.py`, define a new `VerificationAgent`.
  - Equip it with the `VerificationToolkit` and `KhalaMemoryToolkit`.
  - Its instructions will be to critically evaluate the output of other agents and flag any unverifiable claims.
- [ ] **Task 4: Integrate the `VerificationAgent` into the main workflow.**
  - Modify `backend/main.py` in the `/chat` endpoint.
  - After the `DebateCoordinator` produces a result, pass that result to the `VerificationAgent` before returning the final response to the user. This implements the "Cognition Evaluator" pattern.