# Arxiv Analysis: HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems

**ID:** 2411.02959v1
**Date:** 2024-11-05
**Link:** https://arxiv.org/abs/2411.02959v1

## Executive Summary
This paper introduces HtmlRAG, a novel approach for Retrieval-Augmented Generation (RAG) systems that utilizes HTML as the primary format for retrieved knowledge instead of converting it to plain text. The core idea is that HTML retains significant structural and semantic information (e.g., headings, tables, code blocks) that is lost during plain-text conversion, leading to better-informed generation by Large Language Models (LLMs). To address the challenges of increased input length and noise from HTML tags, CSS, and JavaScript, the paper proposes a comprehensive three-step pipeline:
1.  **HTML Cleaning:** A rule-based process to remove irrelevant content like CSS, JavaScript, and comments, and to compress redundant HTML structures.
2.  **Block Tree Construction:** A granularity-adjustable method to parse the HTML into a hierarchical "block tree," which is more coarse-grained and computationally efficient than a full DOM tree.
3.  **Two-Stage Pruning:** A coarse-to-fine pruning strategy that first uses a text-embedding model to remove large, irrelevant blocks, and then employs a fine-tuned generative model to perform a more detailed pruning of the remaining content.
The experimental results across six QA datasets demonstrate the superiority of this HTML-based approach over traditional plain-text RAG systems.

## Idea Brainstorming
*What concepts are potentially useful for THIS project?*
- **Concept 1: HTML as a Primary Data Source for RAG:** The CryptoSentinel project can benefit from directly ingesting and processing HTML from web sources (e.g., crypto news sites, whitepapers, audit reports). This would provide the agents with richer, more structured information than plain text, potentially improving the quality of market analysis and trade decisions.
- **Concept 2: Two-Stage Pruning for Large Documents:** The coarse-to-fine pruning mechanism is a powerful idea for efficiently handling large volumes of text. This could be adapted not just for HTML, but for any large document that the agents need to process, such as lengthy PDF research papers or detailed financial reports.
- **Concept 3: Block Tree Representation:** The concept of a "block tree" offers a good balance between preserving document structure and maintaining computational efficiency. This could be a valuable data structure for representing and reasoning about any large, structured document within the CryptoSentinel system.

## Gap Analysis
*Compare paper concepts vs. `current_codebase`. What do we already have? What is missing?*

| Feature/Concept | Current State (Codebase) | Missing / Needed |
| :--- | :--- | :--- |
| Data Ingestion Format | Assumed to be plain text or structured data from APIs. | An HTML parsing and cleaning module is needed to process web pages. |
| Document Pruning | Likely uses standard text chunking methods. | A sophisticated, two-stage pruning mechanism for large documents is missing. |
| Structural Document Representation | The system likely treats documents as flat text. | A hierarchical representation like the "block tree" is needed to leverage document structure. |
| Web Fetching Tools | Unknown, but likely has some basic HTTP request tools. | A dedicated tool in `backend/tools/` for fetching, cleaning, and pruning web pages would be a valuable addition. |

## Implementation Plan
*Granular, step-by-step task list to port the ideas to our code.*

- [ ] Task 1: **Develop an HTML Processing Module:** Create a new module in `backend/tools/` that handles fetching, cleaning, and parsing HTML content into a structured format. This module would implement the cleaning rules described in the paper (removing CSS/JS, compressing tags).
- [ ] Task 2: **Implement Block Tree Construction:** Within the new module, add functionality to convert a cleaned HTML document into a "block tree" data structure. This will require defining the block tree node structure and the logic for merging DOM nodes.
- [ ] Task 3: **Implement Embedding-Based Pruning:** Add a method to the HTML processing module that performs the first stage of pruning using a text-embedding model to score and filter blocks in the block tree.
- [ ] Task 4: **Integrate into RAG Pipeline:** Update the existing RAG pipeline to use the new HTML processing module. This will likely involve modifying the data ingestion and retrieval logic to handle HTML documents.
- [ ] Task 5: **(Advanced) Implement Generative Pruning:** As a follow-on enhancement, fine-tune a small, efficient generative model (like Phi-3.5-Mini) for the second-stage pruning. This would involve creating a training dataset from crypto-related web pages and training the model to identify the most relevant content blocks.
