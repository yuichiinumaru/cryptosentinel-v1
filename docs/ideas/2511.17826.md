# Arxiv Analysis: Deterministic Inference across Tensor Parallel Sizes

**ID:** 2511.17826
**Date:** 2023-11-27
**Link:** https://arxiv.org/abs/2511.17826

## Executive Summary
The paper introduces Tree-Based Invariant Kernels (TBIK), a method to achieve deterministic, bit-wise identical inference for Large Language Models (LLMs) across different system configurations, particularly varying Tensor Parallel (TP) sizes. The core problem is that floating-point arithmetic is non-associative, causing different reduction orders in distributed GPU computations to yield slightly different results. This non-determinism is a critical issue in applications requiring reproducibility, such as LLM-as-a-judge, multi-agent systems, and Reinforcement Learning (RL). The authors propose aligning the intra- and inter-GPU reduction orders using a unified hierarchical binary tree structure, ensuring the computation order remains consistent regardless of the number of GPUs. Their implementation in Triton, integrated into vLLM and FSDP, demonstrates zero probability divergence and provides a solution for the training-inference mismatch in RL pipelines.

## Idea Brainstorming
The concepts in this paper are highly relevant to the CryptoSentinel project, even though it does not currently use RL or complex tensor parallelism. The value for this project lies in ensuring the reliability and predictability of its multi-agent system.

- **Concept 1: Deterministic Multi-Agent Collaboration:** The CryptoSentinel system is composed of multiple specialized agents that collaborate to make trading decisions (e.g., `MarketAnalyst`, `BullResearcher`, `BearResearcher`, `DebateCoordinator`). Non-deterministic outputs from any of these agents could lead to cascading inconsistencies, making the overall system behavior unpredictable and extremely difficult to debug. For example, a slight change in the `MarketAnalyst`'s output could cause the `DebateCoordinator` to make a different decision, leading to a different trade execution for the exact same market data. Implementing deterministic kernels would ensure that each agent produces the same output for the same input, every single time.

- **Concept 2: Reliable Testing and Evaluation:** One of the prime directives in `AGENTS.md` is the "Law of Verification (TDD)". Reproducibility is the bedrock of effective testing. If the system's outputs are not deterministic, it's impossible to write reliable integration tests that assert specific outcomes. By ensuring deterministic inference, we can create a robust testing suite that validates the behavior of the entire agent team with confidence.

- **Concept 3: Future-Proofing for Scale:** While the current system does not use tensor parallelism, it may need to in the future to handle more sophisticated models or higher throughput. Adopting the principles from this paper now would make the system more robust and scalable. If the project ever incorporates on-policy RL for strategy optimization, the deterministic alignment between training and inference environments proposed in the paper would become critical.

## Gap Analysis
*Compare paper concepts vs. `current_codebase`. What do we already have? What is missing?*

| Feature/Concept | Current State (Codebase) | Missing / Needed |
| :--- | :--- | :--- |
| **Deterministic Inference** | The agent execution is wrapped in `run_in_threadpool`, suggesting a standard, likely non-deterministic execution environment. There is no evidence of custom kernels or specific measures to ensure reproducibility. | The entire concept of deterministic kernels is missing. The system relies on standard library and framework implementations, which are subject to floating-point non-associativity. |
| **Tensor Parallelism (TP)** | The codebase shows no signs of using tensor parallelism. The `get_crypto_trading_team` function creates a team of agents that appear to run on a single process. | The system is not designed for distributed, parallel execution of a single model. This is not a direct gap for the current architecture, but it is a missing capability for future scaling. |
| **Reinforcement Learning (RL) Pipeline** | There is no RL pipeline in the current codebase. The system appears to be a rule-based or model-inference-based trading system, not one that learns from experience. | No training or rollout engine exists. The distinction between `vLLM` and `FSDP` as described in the paper is not applicable to the current system. |
| **Multi-Agent System** | The project has a well-defined multi-agent architecture with specialized agents like `MarketAnalyst`, `Trader`, and a `DebateCoordinator`. | The collaboration between these agents is vulnerable to non-determinism. There is no mechanism to guarantee that the inputs to the `DebateCoordinator` are reproducible, which could lead to inconsistent decisions. |

## Implementation Plan
*Granular, step-by-step task list to port the ideas to our code.*

While a full implementation of TBIK is a significant undertaking and not immediately required for the current architecture, we can propose a phased approach to introduce determinism into the system.

- [ ] **Task 1: Establish a Reproducibility Benchmark:** Create a suite of integration tests that feed a fixed input into the multi-agent system and assert a specific, exact output. This will initially fail due to non-determinism, but it will serve as a benchmark to validate future work.
- [ ] **Task 2: Investigate Batch-Invariant Operations (BIO):** As a first step, research and implement the "Batch-Invariant Operations" mentioned in the paper. While the system doesn't have large batches in the traditional sense, ensuring that operations are invariant to the number of concurrent agent requests would be a good starting point.
- [ ] **Task 3: Isolate Critical Non-Deterministic Components:** Profile the system to identify the most critical components contributing to non-determinism. This is likely to be within the LLM inference calls made by each agent.
- [ ] **Task 4: Develop or Integrate Simplified Deterministic Kernels:** Instead of a full TBIK implementation, we could explore simpler methods to enforce a fixed reduction order in the specific models used by the agents. This might involve using a single GPU and disabling any framework-level optimizations that could introduce non-determinism.
- [ ] **Task 5: Long-Term Research - Custom Triton Kernels:** For a long-term solution, if the project scales to require tensor parallelism, a dedicated effort to implement custom Triton kernels based on the TBIK design would be necessary. This would involve a deeper dive into the model architecture and the underlying GPU operations.
