# Arxiv Analysis: Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models

**ID:** 2511.18177
**Date:** 2024-07-26
**Link:** https://arxiv.org/abs/2511.18177

## Executive Summary
This paper presents a systematic evaluation of different Retrieval-Augmented Generation (RAG) architectures for financial document question answering. It compares a vector-based agentic RAG system against a non-vector, hierarchical node-based reasoning system. The key finding is that the vector-based system achieves a 68% win rate in answer quality over the hierarchical system with comparable latency. The paper also evaluates two advanced RAG techniques: 1) **Cross-Encoder Reranking**, which improved Mean Reciprocal Rank (MRR) by 59% and achieved perfect recall, and 2) **Small-to-Big Retrieval**, which increased answer quality with a 65% win rate at a negligible 0.2-second latency cost. The research demonstrates that advanced RAG techniques provide significant performance gains in the complex financial domain.

## Idea Brainstorming
*What concepts are potentially useful for THIS project?*
- **Cross-Encoder Reranking:** The current system uses vector search for retrieval. Introducing a second-stage reranker, as demonstrated in the paper, could dramatically improve the relevance of the context provided to the agents, leading to more accurate and reliable outputs. This directly addresses the challenge of finding the most precise information in dense financial documents.
- **Small-to-Big Retrieval:** This is a low-cost, high-impact technique. Our agents often deal with complex queries where context is spread across multiple text chunks. Implementing a mechanism to retrieve a small, precise chunk and then expand it with surrounding context could provide the LLM with the necessary information to perform multi-hop reasoning without the "lost in the middle" problem.
- **Systematic Evaluation Framework:** The paper outlines a robust framework for evaluating RAG systems (MRR, Recall@5, LLM-as-a-judge, latency, cost). Adopting a similar methodology would allow us to benchmark our own retrieval and generation pipelines, quantify the impact of changes, and identify bottlenecks.

## Gap Analysis
*Compare paper concepts vs. `current_codebase`. What do we already have? What is missing?*

| Feature/Concept | Current State (Codebase) | Missing / Needed |
| :--- | :--- | :--- |
| **Vector-Based Agentic RAG** | The project has a sophisticated vector-based agentic RAG system, using hybrid search and metadata filtering (as seen in `backend/agents/__init__.py`). | The core architecture is aligned with the paper's best-performing baseline. No gap here. |
| **Cross-Encoder Reranking** | The current retrieval pipeline relies solely on the initial vector search from Azure AI Search. There is no secondary, more computationally intensive reranking step. | A reranking module needs to be integrated. This would involve adding a library like `cohere` and modifying the retrieval workflow to take the top-k results from vector search and re-sort them before passing them to the agent. |
| **Small-to-Big Retrieval** | The system currently retrieves discrete, fixed-size chunks (512 tokens). It does not augment these chunks with their neighbors. | The logic in the retrieval toolkit (`backend/tools/`) would need to be modified to identify and fetch the chunks adjacent to the primary retrieved chunk and then merge them. This requires chunk indexing to be aware of sequence. |
| **Hierarchical Node-Based System**| The project does not use this non-vector approach. | Not a required feature. The paper's findings validate our project's choice to focus on a vector-based architecture, so this is an "anti-gap" - a confirmation of our current design. |

## Implementation Plan
*Granular, step-by-step task list to port the ideas to our code.*

- [ ] **Task 1: Integrate Cross-Encoder Reranking:**
    - [ ] Add `cohere` or a similar library to `backend/requirements.txt`.
    - [ ] In the relevant data retrieval tool, modify the function that fetches context.
    - [ ] Increase the initial `k` value for the vector search (e.g., retrieve 20 chunks).
    - [ ] Pass the retrieved chunks and the original query to the cross-encoder model.
    - [ ] Select the new top-k (e.g., 5) chunks based on the reranker's scores to be used as context.
- [ ] **Task 2: Implement Small-to-Big Retrieval:**
    - [ ] Ensure that when documents are chunked and stored, their sequential order is preserved via metadata (e.g., `chunk_index`).
    - [ ] Modify the retrieval function to perform a secondary lookup after the initial vector search.
    - [ ] For each retrieved chunk, fetch its preceding and succeeding chunks (e.g., `chunk_index - 1` and `chunk_index + 1`).
    - [ ] Implement logic to merge the target and neighbor chunks into a single context block.
- [ ] **Task 3: Develop a RAG Evaluation Suite:**
    - [ ] Create a benchmark dataset of questions with ground-truth context locations (similar to the paper's methodology).
    - [ ] Write evaluation scripts in `backend/tests/` to calculate MRR and Recall@k for our retrieval system.
    - [ ] Implement an "LLM-as-a-judge" evaluation script to programmatically compare the quality of answers generated with and without the new features.