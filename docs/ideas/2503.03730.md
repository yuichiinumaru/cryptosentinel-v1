# Arxiv Analysis: Towards Understanding Distilled Reasoning Models

**ID:** 2503.03730
**Date:** 2024-07-29
**Link:** https://arxiv.org/abs/2503.03730

## Executive Summary
This paper investigates how model distillation affects reasoning abilities in LLMs. The authors use a "sparse crosscoder" to identify and compare features between base models and their distilled counterparts. The key finding is that distilled models develop unique, "reasoning features" corresponding to behaviors like self-reflection ("wait, let me check") and deductive reasoning ("therefore..."). These features are not as prominent or causally effective in the base models.

Crucially, the paper demonstrates that these identified feature directions can be used to steer the model's behavior at inference time. By adding a "self-reflection" feature vector to the model's activations, the model can be pushed into an "over-thinking" mode. Conversely, adding a "deductive" feature vector can steer it into a more confident, "incisive-thinking" mode. The research also suggests that larger distilled models develop more structured internal representations, which correlates with better performance.

## Idea Brainstorming
The most valuable concept for CryptoSentinel is the idea of **steerable reasoning modes**. While implementing a sparse crosscoder to find the exact feature vectors is complex and outside our current scope, we can adopt the high-level principle: dynamically modifying an agent's reasoning style to suit a specific task.

- **Concept 1: Reasoning Profiles.** We can define a set of "reasoning profiles" (e.g., `INcisive`, `Reflective`, `Exploratory`) that modify the system prompts of our agents. This would allow us to "steer" their behavior without changing the underlying model, mimicking the paper's findings at a higher level of abstraction.
- **Concept 2: Dynamic Agent Configuration.** The agent factory could be enhanced to accept a `ReasoningProfile` parameter, allowing the `DebateCoordinator` or a user to spawn agents with specific cognitive styles for a given task. For example, a `Reflective` BearResearcher might be better at finding subtle risks, while an `Incisive` BullResearcher could quickly formulate a strong trade thesis.

## Gap Analysis
Our current system uses a multi-agent "Debate" architecture, which is a great foundation. However, the reasoning style of each agent is static and hard-coded in their respective instruction files.

| Feature/Concept | Current State (Codebase) | Missing / Needed |
| :--- | :--- | :--- |
| **Multi-Agent System** | The `DebateCoordinator` manages `BullResearcher` and `BearResearcher` agents. | The overall structure exists. |
| **Steerable Reasoning** | Agent reasoning is static, defined by fixed prompts in their instruction files. | A mechanism to dynamically alter agent prompts/behavior at runtime is needed. |
| **Reasoning Control** | The system has no explicit concept of different "thinking modes." | We need to define and implement `ReasoningProfile` enums and integrate them into the agent creation process. |
| **Dynamic Instantiation**| The Agent Factory instantiates agents per session. | The factory needs to be extended to accept a reasoning profile and modify the agent's system prompt accordingly. |

## Implementation Plan
This plan focuses on integrating the *idea* of steerable reasoning without implementing the low-level mechanistic interpretability techniques from the paper.

- [ ] **1. Define Reasoning Profiles:**
    - In `backend/agents/profiles.py` (new file), create an `Enum` called `ReasoningProfile` with values like `DEFAULT`, `INCISIVE`, and `REFLECTIVE`.
    - Create a dictionary that maps these enum values to prompt modifiers. For example, `REFLECTIVE` might add a suffix like: "Take a moment to doubt your initial conclusions. Re-evaluate your assumptions and explicitly state any potential flaws in your reasoning before providing your final answer."

- [ ] **2. Enhance Agent Factory:**
    - Modify the `get_crypto_trading_team` function in `backend/agents/__init__.py`.
    - It should accept an optional `reasoning_profile: ReasoningProfile = ReasoningProfile.DEFAULT` argument.
    - Inside the factory, when loading agent instructions, append the corresponding modifier from the profile dictionary to the base system prompt.

- [ ] **3. Expose Reasoning Control via API:**
    - Update the `chat_with_agent` endpoint in `backend/main.py` to accept an optional `reasoning_profile` string in the request body.
    - This string will be converted to the `ReasoningProfile` enum and passed to the agent factory.

- [ ] **4. Documentation:**
    - Update `docs/03-architecture.md` to reflect the new `ReasoningProfile` system and its role in the agent factory.
    - Add a small note in `AGENTS.md` mentioning the availability of different reasoning profiles for experimentation.
